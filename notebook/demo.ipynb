{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration Notebook: CT-AGRG Model Initialization, Loading, and Inference\n",
    "\n",
    "This notebook demonstrates the initialization, loading, and inference procedures for the CT-AGRG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "from step2.modules.utils_dir import parse_yaml\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Script with parameters from JSON file')\n",
    "parser.add_argument('--yaml_file', type=str, default='.step2/config/default.yaml', help='YAML file containing parameters')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.yaml_file = \"./step2/config/default.yaml\"\n",
    "\n",
    "params         = parse_yaml(args.yaml_file)\n",
    "yaml_file      = args.yaml_file\n",
    "args           = argparse.Namespace(**params)\n",
    "args.yaml_file = yaml_file\n",
    "\n",
    "args.device    = torch.device('cuda:2')\n",
    "device         = args.device\n",
    "\n",
    "print(f'Warnings! Device: {device} will be used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize and load CT-AGRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from step2.models.report_generation_model import ReportGenerationModel\n",
    "\n",
    "# Please adjust paths\n",
    "args.path_gpt2         = \"./ckpt/gpt-2-pubmed-medium\"\n",
    "args.path_report_model = \"./ckpt/model_state_dict.pth\"\n",
    "args.path_thresholds   = \"./ckpt/thresholds.json\"\n",
    "\n",
    "model = ReportGenerationModel(args, mode=\"generation\")\n",
    "ckpt  = torch.load(args.path_report_model)\n",
    "msg = model.load_state_dict(ckpt);\n",
    "print(msg)\n",
    "\n",
    "model.to(device);\n",
    "model.freeze();\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def get_tokenizer(args):\n",
    "    \"\"\"\n",
    "    Return GPT-2 tokenizer.\n",
    "    \"\"\"\n",
    "    checkpoint          = args.path_gpt2\n",
    "    tokenizer           = GPT2Tokenizer.from_pretrained(checkpoint)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CT Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nii_img_to_tensor(path, dd=240, dh=480, dw=480):\n",
    "    \"\"\"\n",
    "    Read the volume and pre-process it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Warning: To adjust!\n",
    "    # Assuming that the CT scan is already formatted with SLP orientation with HU values\n",
    "    array = np.load(path)['arr_0'] # [C, H, W]\n",
    "\n",
    "    # Array to tensor\n",
    "    tensor = torch.tensor(array)\n",
    "\n",
    "    # Clip Hounsfield Units to [-1000, +200]\n",
    "    tensor = torch.clip(tensor, -1000., +200.)\n",
    "\n",
    "    # Shift to [0, +1200]\n",
    "    tensor = tensor + torch.tensor(+1000., dtype=torch.float32)\n",
    "\n",
    "    # Map [0, +1200] to [0, 1]\n",
    "    tensor = tensor / torch.tensor(+1200., dtype=torch.float32)\n",
    "\n",
    "    # ImageNet Normalization\n",
    "    tensor = tensor + torch.tensor(-0.449, dtype=torch.float32)\n",
    "\n",
    "    # extract dimensions\n",
    "    d, h, w = tensor.shape\n",
    "\n",
    "    # calculate cropping values for height, width, and depth\n",
    "    h_start = max((h - dh) // 2, 0)\n",
    "    h_end   = min(h_start + dh, h)\n",
    "    w_start = max((w - dw) // 2, 0)\n",
    "    w_end   = min(w_start + dw, w)\n",
    "    d_start = max((d - dd) // 2, 0)\n",
    "    d_end   = min(d_start + dd, d)\n",
    "\n",
    "    # crop\n",
    "    tensor = tensor[d_start:d_end, h_start:h_end, w_start:w_end]\n",
    "\n",
    "    # # calculate padding values for height, width, and depth\n",
    "    pad_h_before = (dh - tensor.size(1)) // 2\n",
    "    pad_h_after  = dh - tensor.size(1) - pad_h_before\n",
    "    pad_w_before = (dw - tensor.size(2)) // 2\n",
    "    pad_w_after  = dw - tensor.size(2) - pad_w_before\n",
    "    pad_d_before = (dd - tensor.size(0)) // 2\n",
    "    pad_d_after  = dd - tensor.size(0) - pad_d_before\n",
    "\n",
    "    # pad\n",
    "    tensor = torch.nn.functional.pad(tensor, (pad_w_before, pad_w_after, pad_h_before, pad_h_after, pad_d_before, pad_d_after), value=-0.449)\n",
    "\n",
    "    # unsqueeze\n",
    "    tensor = tensor.unsqueeze(0) # [1, 240, 480, 480]\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please adjust with your own path.\n",
    "path_volume = \"/path/to/volume\"\n",
    "volume      = nii_img_to_tensor(path_volume) # [1, 240, 480, 480]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference to generate the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract generated report as a string\n",
    "generated_report = model.generate(\n",
    "    tokenizer            = tokenizer, \n",
    "    volumes              = volume.unsqueeze(0), \n",
    "    max_length           = args.max_seq_length,\n",
    "    num_beams            = args.beam_size, \n",
    "    num_beam_groups      = args.group_size,\n",
    "    do_sample            = args.do_sample,\n",
    "    num_return_sequences = args.num_return_sequences,\n",
    "    early_stopping       = args.early_stopping\n",
    ")\n",
    "\n",
    "print(generated_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpzenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
